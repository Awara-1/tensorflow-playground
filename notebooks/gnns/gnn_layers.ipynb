{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementation of \"Deep Residual Learning for Image Recognition\" paper - https://iopscience.iop.org/article/10.1088/1742-6596/1871/1/012071/meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads:int, hidden_size:int, max_neighbors:int):\n",
    "        super(GraphAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_neighbors = max_neighbors\n",
    "        self.num_heads = num_heads\n",
    "        # trainable alignment matrix for general-style self-attention\n",
    "        self.attn_ws = [tf.keras.layers.Dense(hidden_size) for _ in range(num_heads)]\n",
    "        # output layer\n",
    "        self.out_w = tf.keras.layers.Dense(hidden_size)\n",
    "\n",
    "\n",
    "    def call(self, inputs: dict, debug=False):\n",
    "        \"\"\"Calculate scores of all neighboring nodes using general Luong-style\n",
    "        self-attention. If multiple attention heads are used, contexts are\n",
    "        concatenated.\n",
    "\n",
    "        # Inputs:\n",
    "            value: the vector relating to the node to encode. Should be of size:\n",
    "              [batch_size, 1, hidden_size]\n",
    "            query: a tensor providing embeddings for the neighbors of the node\n",
    "              which are to be attended to. Each query should be of size:\n",
    "              [batch_size, max_neighbors, hidden_size]\n",
    "            num_neighbors: the number of neighbors to attend to from the query\n",
    "              tensor. Only the first `max_neighbors` nodes are attended to.\n",
    "        \"\"\"\n",
    "\n",
    "        value = inputs['value']\n",
    "        query = inputs['query']\n",
    "        num_neighbors = inputs['num_neighbors']\n",
    "\n",
    "        assert value.shape[1] == 1, f'second dim of value should be 1, but was {value.shape[1]}'\n",
    "        assert query.shape[1] == self.max_neighbors, f'second dim of query should equal max_neighbors, but was {query.shape[1]}'\n",
    "        assert num_neighbors < self.max_neighbors, f'num_neighbors input of {num_neighbors} cannot be greater than max neighbors'\n",
    "\n",
    "        # aggregate features from all neighbors, including the node itself\n",
    "        query = tf.concat([value, query], axis=1)\n",
    "\n",
    "        # multi-head self-attention\n",
    "        contexts = []\n",
    "        for i in range(self.num_heads):\n",
    "            query = self.attn_ws[i](query)\n",
    "            e = tf.matmul(value, query, transpose_b=True)\n",
    "            e = tf.nn.swish(e)\n",
    "            # apply mask before softmaxing\n",
    "            mask = tf.sequence_mask(num_neighbors + 1, maxlen=self.max_neighbors)[:, tf.newaxis]\n",
    "            e = tf.where(mask, e, tf.ones_like(e) * -1e9)\n",
    "            scores = tf.nn.softmax(e)\n",
    "            # sum all query embeddings according to attention scores\n",
    "            context = tf.matmul(scores, query)\n",
    "            contexts.append(context)\n",
    "\n",
    "        # concatenate contexts from each attention head\n",
    "        contexts = tf.concat(contexts, axis=-1)\n",
    "\n",
    "        # produce new features from full context\n",
    "        x = self.out_w(contexts)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = tf.random.normal([8, 1, 512])\n",
    "query = tf.random.normal([8, 10, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_attn = GraphAttention(num_heads=4, hidden_size=512, max_neighbors=10)\n",
    "x = g_attn({\n",
    "  'query': query,\n",
    "  'value': value,\n",
    "  'num_neighbors': 5\n",
    "})\n",
    "\n",
    "print(f'Input shape: {value.shape}')\n",
    "print(f'Output shape: {x.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNN Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionNetwork(tf.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                num_heads: int,\n",
    "                hidden_sizes: int,\n",
    "                num_neighbors: int,\n",
    "                max_neighbors: int):\n",
    "        super(GraphAttentionNetwork, self)\n",
    "        \n",
    "        self.num_neighbors = num_neighbors\n",
    "        \n",
    "        self.gat1 = GraphAttention(num_heads, hidden_sizes, max_neighbors)\n",
    "        self.gat2 = GraphAttention(num_heads, hidden_sizes, max_neighbors)\n",
    "        self.gat3 = GraphAttention(num_heads, hidden_sizes, max_neighbors)\n",
    "        self.gat4 = GraphAttention(num_heads, hidden_sizes, max_neighbors)\n",
    "        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.mlp = tf.keras.layers.Dense(4) # 4 classification\n",
    "\n",
    "    def __call__(self, inputs: dict):\n",
    "        \n",
    "        x = self.gat1(inputs=inputs)\n",
    "        temp_1 = x\n",
    "        \n",
    "        inputs_2 = {\n",
    "            'query': x,\n",
    "            'value': inputs['value'],\n",
    "            'num_neighbors': self.num_neighbors\n",
    "        }\n",
    "        x = self.gat2(inputs_2)\n",
    "        temp_2 = x\n",
    "        \n",
    "        inputs_3 = {\n",
    "            'query': x,\n",
    "            'value': inputs['value'],\n",
    "            'num_neighbors': self.num_neighbors\n",
    "        }\n",
    "        x = self.gat3(inputs_3)\n",
    "        temp_3 = x\n",
    "        \n",
    "        x = tf.add(temp_1, temp_3)\n",
    "        \n",
    "        inputs_4 = {\n",
    "            'query': x,\n",
    "            'value': inputs['value'],\n",
    "            'num_neighbors': self.num_neighbors\n",
    "        }\n",
    "        x = self.gat4(inputs_4)\n",
    "        temp4 = x\n",
    "        \n",
    "        x = tf.concat([temp_1, temp_2, temp_3, x], 1) # concat on axis 1, axis 0 increases the batch\n",
    "        # dimensions which is not desired\n",
    "        \n",
    "        x = self.flatten(x) # flatten before passing in MLP\n",
    "        \n",
    "        x = self.mlp(x)\n",
    "        \n",
    "        x = tf.nn.softmax(x)\n",
    "        \n",
    "        print(x.shape)  \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gat_network = GraphAttentionNetwork(num_heads=4, hidden_sizes=512, num_neighbors=5, max_neighbors=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_test = {\n",
    "  'query': query,\n",
    "  'value': value,\n",
    "  'num_neighbors': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_test = gat_network(inputs=in_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "- figure out what num_heads is\n",
    "- figure out how all the parameters relate to the graph representation of an image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-mlflow-examples",
   "language": "python",
   "name": "env-mlflow-examples"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
