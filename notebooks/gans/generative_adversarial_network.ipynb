{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implement a GAN from the following paper: https://arxiv.org/abs/1406.2661"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(np.array([[1, 2, 3, 4],\n",
    "                         [1, 3, 5, 6],\n",
    "                         [2, 5, 6, 7],\n",
    "                         [3, 5, 7, 8]\n",
    "                         ]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(tf.Module):\n",
    "    \n",
    "    def __init__(self, out_features, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.is_built = False # is built flag for dynamic input size inference\n",
    "        self.out_features = out_features\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        if not self.is_built:\n",
    "            self.w = tf.Variable(\n",
    "                tf.random.normal([x.shape[-1], self.out_features]), name='w')\n",
    "            self.b = tf.Variable(tf.zeros([self.out_features]), name='b')\n",
    "            self.is_built = True\n",
    "        \n",
    "        x_hat = tf.matmul(x, self.w) + self.b\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_test = Dense(out_features=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_test.__call__(x=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discriminator takes in as input, the output of the Generator that creates the \"fake\" image reconctruction from noisy data. Output of the discriminator is a probability prediction of 1 or 0 if image is from data (i.e. 1) or from the generator (i.e 0 and hence \"fake\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.dense_1 = Dense(out_features=10000)\n",
    "        self.dense_2 = Dense(out_features=1)\n",
    "       \n",
    "    def __call__(self, x):\n",
    "        x = self.dense_1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.dense_2(x)\n",
    "        return tf.nn.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discr = Discriminator(name=\"discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discr.__call__(x=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generator takes in random noise and outputs an image tensor which is same size as the real image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_img = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_noise = tf.constant(np.array([[1, 2, 3, 4, 9],\n",
    "                         [1, 3, 5, 6, 20],\n",
    "                         [2, 5, 6, 7, 43],\n",
    "                         [3, 5, 7, 8, 30]\n",
    "                         ]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.dense_1 = Dense(out_features=10000)\n",
    "        self.dense_2 = Dense(out_features=4000)\n",
    "        self.dense_3 = Dense(out_features=flat_img)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        x = self.dense_1(x)\n",
    "        x = tf.nn.leaky_relu(x) # leaky_relu good for vanishing gradients\n",
    "        x = self.dense_2(x)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        return self.dense_3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(name=\"generator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen(x=rnd_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function & Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_test = np.array([1, 0, 1, 1])\n",
    "y_pred_test = np.array([1, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_d = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, name=\"discr_opt\")\n",
    "opt_g = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, name=\"opt_opt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_true, y_pred):\n",
    "    \n",
    "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, 1)), dtype=tf.float32)\n",
    "    y_pred_tf = tf.cast(tf.reshape(y_pred, (-1, 1)), dtype=tf.float32)\n",
    "    \n",
    "    return tf.compat.v1.losses.sigmoid_cross_entropy(y_true_tf, y_pred_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(y_true=y_true_test, y_pred=y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "discr_epochs = 3\n",
    "gen_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    # discriminator training\n",
    "    for k in range(discr_epochs):\n",
    "        print(f\"Discr Epoch nr is: {k}\")\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            pred_discr = discr(x=x)\n",
    "            loss_discr = loss(y_true=y, y_pred=pred_discr)\n",
    "        # do backprop\n",
    "        grads = tape.gradient(loss_discr, discr.trainable_variables)\n",
    "        opt_d.apply_gradients(zip(grads, discr.trainable_variables))\n",
    "        print(f\"Current discr loss from real input is {tf.reduce_mean(loss_discr)}\")\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            pred_gen = gen(x=rnd_noise)\n",
    "            pred_discr_from_gen = discr(pred_gen)\n",
    "            loss_gen = loss(y_true=y, y_pred=pred_discr_from_gen)\n",
    "        # do backprop\n",
    "        grads = tape.gradient(loss_gen, discr.trainable_variables)\n",
    "        opt_d.apply_gradients(zip(grads, discr.trainable_variables))\n",
    "        print(f\"Current discr loss from fake input is {loss_gen}\")\n",
    "    \n",
    "    # generator training\n",
    "    for i in range(gen_epochs):\n",
    "        print(f\"Gen Epoch nr is: {i}\")\n",
    "              \n",
    "        with tf.GradientTape() as tape:\n",
    "            pred_gen = gen(x=rnd_noise)\n",
    "            print(f\"pred_gen is: {pred_gen}\")\n",
    "            print(f\"pred discr is: {discr(pred_gen)}\")\n",
    "#             loss_gen = loss(y_true=x, y_pred=pred_gen) # use standard optimizer\n",
    "            loss_gen = 1 - discr(pred_gen) # using loss in the paper instead\n",
    "        # do backprop\n",
    "        grads = tape.gradient(loss_gen, gen.trainable_variables)\n",
    "        opt_g.apply_gradients(zip(grads, gen.trainable_variables))\n",
    "        print(f\"Current gen loss is {tf.reduce_mean(loss_gen)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-rr-denoising-tf2",
   "language": "python",
   "name": "env-rr-denoising-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
