{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use to develop any generic DNN based on input of layers and their nr of neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with Tensorflow Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.array([[1, 2, 4], [4, 5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_array = tf.Variable(array).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_mean(array, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Neural Network Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight and bias experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(np.array([1, 2, 3, 4]), dtype=tf.float32)\n",
    "y = tf.constant(np.array([2, 4, 6, 8]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights and bias with random tensor values\n",
    "w = tf.Variable(np.random.randn(), dtype=\"float32\")\n",
    "b = tf.Variable(np.random.randn(), dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = [4, 3, 3, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ran1 = tf.random.normal([n_layers[0], n_layers[1]], stddev=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ran1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = tf.Variable(ran1, name='W1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = tf.zeros([1, n_layers[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_in = tf.Variable(tf.random.normal([1, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1 = tf.matmul(x_in, w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(np.array([[1, 2, 3, 4],\n",
    "                         [1, 3, 5, 6],\n",
    "                         [2, 5, 6, 7],\n",
    "                         [3, 5, 7, 8]\n",
    "                         ]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([1, 2, 3, 3])\n",
    "shape = (data.size, data.max()+1)\n",
    "one_hot = np.zeros(shape)\n",
    "rows = np.arange(data.size)\n",
    "y = one_hot[rows, data] = 1\n",
    "y = one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericNeuralNetwork:\n",
    "    \n",
    "    \"\"\"\n",
    "    This class builds any neural network for any given number of layers and their neuron units\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_layers: list):\n",
    "        \"\"\"\n",
    "        constructor\n",
    "        :param n_layers: list of layers of neural network from input to output containing number of nodes/units\n",
    "        in each layer of the network\n",
    "        \"\"\"\n",
    "        \n",
    "        # store the parametrs of network\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.params = [] # store alternative weights and biases\n",
    "        \n",
    "        # declare layer-wise weights and biases. NOTE: each layer weight matrix in layer j \n",
    "        # = num_in_layer_j-1 * num_in_layer_j\n",
    "        for i in range(0, len(n_layers)-1, 1):\n",
    "            w_x = tf.Variable(tf.random.normal([n_layers[i], n_layers[i+1]], stddev=0.1), name=f'W{i+1}')\n",
    "            b_x = tf.Variable(tf.zeros([1, n_layers[i+1]]), name=f'B{i+1}')\n",
    "            self.weights.append(w_x)\n",
    "            self.biases.append(b_x)\n",
    "        self.params = [w for b in zip(self.weights, self.biases) for w in b]\n",
    "         \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the neural network\n",
    "        :pararm x: input data\n",
    "        :return: predicted logits\n",
    "        \"\"\"\n",
    "        Z_list = [] # store all layer's activation outputs\n",
    "\n",
    "        # calculate first layer output from inputs\n",
    "        X_tf = tf.cast(x, dtype=tf.float32)\n",
    "        Z1 = tf.matmul(X_tf, self.weights[0]) + self.biases[0]\n",
    "        Z1 = tf.nn.relu(Z1)\n",
    "\n",
    "        Z_list.append(Z1)\n",
    "\n",
    "        # for remaining layers compute all activation outputs\n",
    "        j = 0 # counter for Z_list\n",
    "        for i in range(1, len(self.weights), 1): # first layer already computed so start at index 1\n",
    "            Z_i = tf.matmul(Z_list[j], self.weights[i]) + self.biases[i]\n",
    "            if i != len(self.weights) - 1: # NOTE: for last layer, we do not apply an activation (for logits)\n",
    "                Z_i = tf.nn.relu(Z_i)  \n",
    "            Z_list.append(Z_i)\n",
    "            j += 1\n",
    "\n",
    "        return Z_list[-1] # logit layer (final layer before activation)\n",
    "    \n",
    "    \n",
    "    def loss(self, y_true, logits):\n",
    "        \"\"\"\n",
    "        logits = Tensor of shape (batch_size, size_output)\n",
    "        y_true = Tensor of shape (batch_size, size_output)\n",
    "        \"\"\"\n",
    "        y_true_tf = tf.cast(tf.reshape(y_true, (-1, 1)), dtype=tf.float32)\n",
    "        logits_tf = tf.cast(tf.reshape(logits, (-1, 1)), dtype=tf.float32)\n",
    "        return tf.compat.v1.losses.sigmoid_cross_entropy(y_true_tf, logits_tf)\n",
    "    \n",
    "    \n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"\n",
    "        Back propagation algorithm to calculate weights and biases\n",
    "        :param x: input data\n",
    "        :param y: true labeled data\n",
    "        \"\"\"\n",
    "        optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "        with tf.GradientTape() as tape:\n",
    "            predicted = self.forward(x)\n",
    "            current_loss = self.loss(y_true=y, logits=predicted)\n",
    "        grads = tape.gradient(current_loss, self.params)\n",
    "        optimizer.apply_gradients(zip(grads, self.params),\n",
    "                                 global_step=tf.compat.v1.train.get_or_create_global_step())\n",
    "        print(f\"Current loss {tf.reduce_mean(current_loss)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize DNN class\n",
    "generic_neural_network = GenericNeuralNetwork(n_layers=[4, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_layer_output = generic_neural_network.forward(x=x)\n",
    "logit_layer_output_relu = tf.nn.sigmoid(logit_layer_output)\n",
    "logit_layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "for e in range(num_epochs):\n",
    "    generic_neural_network.backprop(x=x, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_neural_network.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = generic_neural_network.forward(x=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = tf.nn.sigmoid(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-rr-denoising-tf2",
   "language": "python",
   "name": "env-rr-denoising-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
