{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementation of \"Speech enhancement by LSTM-based noise suppression followed by CNN-based speech restoration\" paper - https://link.springer.com/article/10.1186/s13634-020-00707-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import typing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's assume we have an input of [batch_size, sequence_length, nr_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (1, 1024, 2)\n",
    "x = tf.random.normal(input_shape)\n",
    "x = tf.constant(x, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup individual layers and test to experiment and test that they work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(tf.Module):\n",
    "    \n",
    "    def __init__(self, out_features, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.is_built = False # is built flag for dynamic input size inference\n",
    "        self.out_features = out_features\n",
    "        \n",
    "    def __call__(self, x_in):\n",
    "        if not self.is_built:\n",
    "            self.w = tf.Variable(\n",
    "                tf.random.normal([x.shape[-1], self.out_features]), name='w')\n",
    "            self.b = tf.Variable(tf.zeros([self.out_features]), name='b')\n",
    "            self.is_built = True\n",
    "        \n",
    "        x_hat = tf.matmul(x, self.w) + self.b\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer = Dense(out_features=425)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer_output = dense_layer(x)\n",
    "dense_layer_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(tf.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 num_units: typing.Union[typing.List[int], int],\n",
    "                 use_peepholes: bool,\n",
    "                 name: str = None):\n",
    "        super(LSTM, self).__init__(name)\n",
    "        \n",
    "        # if num_units is given as int, ensure it's set to a list\n",
    "        if isinstance(num_units, int):\n",
    "            num_units = [num_units]\n",
    "        else:\n",
    "            num_units = num_units      \n",
    "        \n",
    "        self.num_units = num_units\n",
    "        self.use_peepholes = use_peepholes\n",
    "        \n",
    "        self.lstm_layers = [tf.compat.v1.nn.rnn_cell.LSTMCell(num_units=size, \n",
    "                                                              use_peepholes=self.use_peepholes) for size in self.num_units]\n",
    "        self.multi_lstm_cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell(self.lstm_layers)\n",
    "                \n",
    "    def __call__(self, x_in):\n",
    "        \n",
    "        return tf.compat.v1.nn.dynamic_rnn(cell=self.multi_lstm_cell, inputs=x_in, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_layer = LSTM(num_units=425, use_peepholes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_layer_output, lstm_layer_state = lstm_layer(x_in=dense_layer_output)\n",
    "lstm_layer_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1D(tf.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "               nr_filters: int,\n",
    "               kernel: int,\n",
    "               stride: int,\n",
    "               use_bias: bool,\n",
    "               name = None\n",
    "              ):\n",
    "        \n",
    "        super(Conv1D, self).__init__(name)\n",
    "        \n",
    "        self.nr_filters = nr_filters\n",
    "        self.kernel = kernel\n",
    "        self.stride = stride\n",
    "        self.use_bias = use_bias\n",
    "        \n",
    "        self.is_built: bool = False\n",
    "        \n",
    "        self.W: tf.Tensor = None \n",
    "        self.b: tf.Tensor = None\n",
    "            \n",
    "    def __call__(self, x_in):\n",
    "        \n",
    "        if not self.is_built:\n",
    "            in_channels = x_in.shape[-1]\n",
    "            filter_weights_shape = (self.kernel, in_channels, self.nr_filters)\n",
    "            \n",
    "            self.W = tf.Variable(tf.random.normal(filter_weights_shape, stddev=0.1),\n",
    "                                trainable=True,\n",
    "                                dtype = tf.float32,\n",
    "                                name = \"conv1d_filters\")\n",
    "            if self.use_bias:\n",
    "                self.b = tf.Variable(tf.random.normal([self.nr_filters]))\n",
    "            \n",
    "            self.is_built = True\n",
    "            \n",
    "        if self.use_bias:\n",
    "            return tf.add(\n",
    "                tf.nn.conv1d(\n",
    "                input=x_in,\n",
    "                filters=self.W,\n",
    "                stride=self.stride,\n",
    "                padding=\"SAME\"\n",
    "                ),\n",
    "                self.b,\n",
    "                name=\"conv1d_layer_with_bias\"\n",
    "            )\n",
    "        else:\n",
    "            return tf.nn.conv1d(\n",
    "                input=x_in,\n",
    "                filters=self.W,\n",
    "                stride=self.stride,\n",
    "                padding=\"SAME\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1d_layer = Conv1D(nr_filters=88, kernel=24, stride=1, use_bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1d_layer(x_in=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pooling_1d = tf.keras.layers.MaxPool1D(pool_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pooling_1d(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv1D Transpose "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1d_transpose_layer = tf.keras.layers.Conv1DTranspose(filters=2*88, kernel_size=24, strides=1, use_bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1d_transpose_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1d_transpose_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampling_layer = tf.keras.layers.UpSampling1D(size=2)\n",
    "upsampling_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Suppression Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The noise suppression module makes use of FF and LSTM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseSuppressor(tf.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                output_size: int,\n",
    "                name: str = None):\n",
    "        super(NoiseSuppressor, self).__init__(name)\n",
    "        \n",
    "        self.dense_layer_1 = Dense(out_features=425)\n",
    "        self.lstm_layer_1 = LSTM(num_units=425, use_peepholes=True)\n",
    "        self.lstm_layer_2 = LSTM(num_units=425, use_peepholes=True)\n",
    "        self.dense_layer_2 = Dense(out_features=425)\n",
    "        self.dense_layer_3 = Dense(out_features=425)\n",
    "        self.dense_layer_4 = Dense(out_features=output_size)\n",
    "        \n",
    "    def __call__(self, x_in: tf.Tensor):\n",
    "        \n",
    "        dense_layer_1 = self.dense_layer_1(x_in=x_in)\n",
    "        dense_layer_1 = tf.nn.relu(dense_layer_1)\n",
    "        lstm_layer_1, _ = self.lstm_layer_1(x_in=dense_layer_1)\n",
    "        lstm_layer_2, _ = self.lstm_layer_2(x_in=lstm_layer_1)\n",
    "        dense_layer_2 = self.dense_layer_2(x_in=lstm_layer_2)\n",
    "        dense_layer_2 = tf.nn.relu(dense_layer_2)\n",
    "        dense_layer_3 = self.dense_layer_3(x_in=dense_layer_2)\n",
    "        dense_layer_3 = tf.nn.relu(dense_layer_3)\n",
    "        dense_layer_4 = self.dense_layer_4(x_in=dense_layer_3)\n",
    "        dense_layer_4 = tf.math.tanh(dense_layer_4)\n",
    "        \n",
    "        return dense_layer_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_suppressor = NoiseSuppressor(output_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = noise_suppressor(x_in=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech Restoration Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The speech restoration module makes use of con1d, conv1d transpose, max pooling and upsampling layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Below is the class for the first block in figure 3 of the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechRestorationNetworkBlock1(tf.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                nr_conv_filters: int,\n",
    "                conv_filter_size: int,\n",
    "                name: str = None\n",
    "                ):\n",
    "        super(SpeechRestorationNetworkBlock1, self).__init__(name)\n",
    "        \n",
    "        self.conv1D_1 = Conv1D(nr_filters=nr_conv_filters, kernel=conv_filter_size, stride=1, use_bias=True)\n",
    "        self.conv1D_2 = Conv1D(nr_filters=nr_conv_filters, kernel=conv_filter_size, stride=1, use_bias=True)\n",
    "        \n",
    "        self.max_pooling_1 = tf.keras.layers.MaxPool1D(pool_size=2)\n",
    "        self.conv1D_3 = Conv1D(nr_filters=2*nr_conv_filters, kernel=conv_filter_size, stride=1, use_bias=True)\n",
    "        self.conv1D_4 = Conv1D(nr_filters=2*nr_conv_filters, kernel=conv_filter_size, stride=1, use_bias=True)\n",
    "        \n",
    "        self.max_pooling_2 = tf.keras.layers.MaxPool1D(pool_size=2)\n",
    "        \n",
    "        self.conv1D_5 = Conv1D(nr_filters=nr_conv_filters, kernel=conv_filter_size, stride=1, use_bias=True)\n",
    "        self.upsampling_1 = tf.keras.layers.UpSampling1D(size=2)\n",
    "        \n",
    "        self.conv1D_6 = Conv1D(nr_filters=2*nr_conv_filters, kernel=conv_filter_size, stride=1, use_bias=True)\n",
    "        self.conv1D_7 = Conv1D(nr_filters=2*nr_conv_filters, kernel=conv_filter_size, stride=1, use_bias=True)\n",
    "        \n",
    "        self.upsampling_2 = tf.keras.layers.UpSampling1D(size=2)\n",
    "        self.conv1D_8 = Conv1D(nr_filters=nr_conv_filters, kernel=conv_filter_size, stride=1, use_bias=True)\n",
    "        self.conv1D_9 = Conv1D(nr_filters=nr_conv_filters, kernel=conv_filter_size, stride=1, use_bias=True)\n",
    "\n",
    "        self.conv1D_last = Conv1D(nr_filters=2, kernel=conv_filter_size, stride=1, use_bias=True)\n",
    "        \n",
    "    \n",
    "    def __call__(self, x_in: tf.Tensor):\n",
    "        \n",
    "        x = self.conv1D_1(x_in)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        x = self.conv1D_2(x) # this will be addeed at a later stage in model\n",
    "        temp1 = tf.nn.leaky_relu(x)\n",
    "        x = self.max_pooling_1(temp1)\n",
    "        x = self.conv1D_3(x)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        x = self.conv1D_4(x) # this will be added at a later stage in model\n",
    "        temp2 = tf.nn.leaky_relu(x)\n",
    "        x = self.max_pooling_2(temp2)\n",
    "        x = self.conv1D_5(x)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        x = self.upsampling_1(x)\n",
    "        x = self.conv1D_6(x)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        x = self.conv1D_7(x)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        add_1 = tf.add(x, temp2)\n",
    "        x = self.upsampling_2(add_1)\n",
    "        x = self.conv1D_8(x)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        x = self.conv1D_9(x)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        add_2 = tf.add(x, temp1)\n",
    "        x = self.conv1D_last(add_2)\n",
    "        return tf.nn.leaky_relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_restoration_network_block1 = SpeechRestorationNetworkBlock1(nr_conv_filters=88, conv_filter_size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block1_output = speech_restoration_network_block1(x_in=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Below is the class for the first block in figure 4 of the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechRestorationNetworkBlock2(tf.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                nr_conv_filters: int,\n",
    "                conv_filter_size: int,\n",
    "                name: str = None\n",
    "                ):\n",
    "        super(SpeechRestorationNetworkBlock2, self).__init__(name)\n",
    "        \n",
    "        self.conv1D_1 = Conv1D(nr_filters=nr_conv_filters, kernel=conv_filter_size, stride=1, use_bias=True)\n",
    "        self.conv1D_2 = Conv1D(nr_filters=nr_conv_filters, kernel=conv_filter_size, stride=2, use_bias=True)\n",
    "        \n",
    "        self.conv1D_3 = Conv1D(nr_filters=2*nr_conv_filters, kernel=conv_filter_size, stride=1, use_bias=True)\n",
    "        self.conv1D_4 = Conv1D(nr_filters=2*nr_conv_filters, kernel=conv_filter_size, stride=2, use_bias=True)\n",
    "        \n",
    "        self.conv1D_5 = Conv1D(nr_filters=nr_conv_filters, kernel=conv_filter_size, stride=1, use_bias=True)\n",
    "        self.conv1D_transpose_1 = tf.keras.layers.Conv1DTranspose(filters=2*nr_conv_filters, kernel_size=conv_filter_size, strides=2, padding=\"same\") # padding must be set to same to ensure output shape = input shape\n",
    "        \n",
    "        self.conv1D_transpose_2 = tf.keras.layers.Conv1DTranspose(filters=2*nr_conv_filters, kernel_size=conv_filter_size, strides=1, padding=\"same\")\n",
    "        self.conv1D_transpose_3 = tf.keras.layers.Conv1DTranspose(filters=nr_conv_filters, kernel_size=conv_filter_size, strides=2, padding=\"same\")\n",
    "        \n",
    "        self.conv1D_transpose_4 = tf.keras.layers.Conv1DTranspose(filters=nr_conv_filters, kernel_size=conv_filter_size, strides=1, padding=\"same\")\n",
    "        \n",
    "        self.conv1D_last = Conv1D(nr_filters=2, kernel=conv_filter_size, stride=1, use_bias=True)\n",
    "        \n",
    "    def __call__(self, x_in: tf.Tensor):\n",
    "        x = self.conv1D_1(x_in)\n",
    "        temp1 = tf.nn.leaky_relu(x)\n",
    "        x = self.conv1D_2(temp1)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        x = self.conv1D_3(x)\n",
    "        temp2 = tf.nn.leaky_relu(x)\n",
    "        x = self.conv1D_4(temp2)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        x = self.conv1D_5(x)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        x = self.conv1D_transpose_1(x)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        add_1 = tf.add(x, temp2)\n",
    "        x = self.conv1D_transpose_2(add_1)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        x = self.conv1D_transpose_3(x)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        add_2 = tf.add(x, temp1)\n",
    "        x = self.conv1D_transpose_4(add_2)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        x = self.conv1D_last(x)\n",
    "        return tf.nn.leaky_relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_restoration_network_block2 = SpeechRestorationNetworkBlock2(nr_conv_filters=88, conv_filter_size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_restoration_network_block2(x_in=block1_output) # input to block 2 is output from block 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-mlflow-examples",
   "language": "python",
   "name": "env-mlflow-examples"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
